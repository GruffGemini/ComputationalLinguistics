{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3a1065",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249dfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "punctuation = string.punctuation + '—«»…'\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c44da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = [token.strip(punctuation) for token in text.lower().split()]\n",
    "    return [morph.parse(word)[0].normal_form for word in tokens if word] # добавляем лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3428a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, embeddings):\n",
    "    similar = [id2word[i] for i in \n",
    "               cosine_distances(\n",
    "                   embeddings[word2id[word]].reshape(1, -1), \n",
    "                   embeddings).argsort()[0][:10]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "wiki = open('wiki_data.txt', encoding='utf-8-sig')\n",
    "preprocessed_texts = [preprocess(text) for text in wiki]\n",
    "with open('preprocessed_texts.pickle', 'wb') as f:\n",
    "    pickle.dump(preprocessed_texts, f)\n",
    "for text in preprocessed_texts:\n",
    "    vocab.update(text)\n",
    "filtered_vocab = [word for word in vocab if vocab[word] > 30]\n",
    "word2id = {word: order + 1 for order, word in enumerate(filtered_vocab)}\n",
    "word2id['PAD'] = 0\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "sentences = []\n",
    "for text in preprocessed_texts:\n",
    "    ids = [word2id[token] for token in text if token in word2id]\n",
    "    sentences.append(ids)\n",
    "vocab_size = len(id2word)\n",
    "sum_occurences = sum(vocab[i] for i in filtered_vocab)\n",
    "word_probs = [vocab[word] / sum_occurences for word in filtered_vocab] # вероятности для выбора негативных примеров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d28aa",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ca91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches_skip_gram(sentences, window, batch_size=1000):\n",
    "    while True:\n",
    "        X_target = []\n",
    "        X_context = []\n",
    "        y = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            for i in range(len(sent) - 1):\n",
    "                word = sent[i]\n",
    "                context = (sent[max(0, i - window // 2):i] + \n",
    "                           sent[i + 1:i + window // 2]) # окно разбивается на две половины\n",
    "                for context_word in context:\n",
    "                    X_target.append(word)\n",
    "                    X_context.append(context_word)\n",
    "                    y.append(1)\n",
    "\n",
    "                    X_target.append(word)\n",
    "                    X_context.append(word2id[np.random.choice(\n",
    "                        filtered_vocab, p=word_probs)]) # негативные примеры выбираются пропорционально вероятностям\n",
    "                    y.append(0)\n",
    "\n",
    "                    if len(X_target) >= batch_size:\n",
    "                        X_target = np.array(X_target)\n",
    "                        X_context = np.array(X_context)\n",
    "                        y = np.array(y)\n",
    "                        yield ((X_target, X_context), y)\n",
    "                        X_target = []\n",
    "                        X_context = []\n",
    "                        y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c57193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_target = tf.keras.layers.Input(shape=(1,))\n",
    "inputs_context = tf.keras.layers.Input(shape=(1,))\n",
    "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
    "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
    "target = tf.keras.layers.Flatten()(embeddings_target)\n",
    "context = tf.keras.layers.Flatten()(embeddings_context)\n",
    "dot = tf.keras.layers.Dot(1)([target, context])\n",
    "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
    "sg_model = tf.keras.Model(inputs=[inputs_target, inputs_context], \n",
    "                       outputs=outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "sg_model.compile(optimizer=optimizer,\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d73082fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2000/2000 [==============================] - 5339s 3s/step - loss: 0.6816 - accuracy: 0.5472 - val_loss: 0.6712 - val_accuracy: 0.5910\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - 5878s 3s/step - loss: 0.6689 - accuracy: 0.5745 - val_loss: 0.6703 - val_accuracy: 0.5894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13617143850>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.fit(gen_batches_skip_gram(sentences, window=12), # размер окна изменён\n",
    "             validation_data=gen_batches_skip_gram(sentences,  window=12),\n",
    "             batch_size=1000,\n",
    "             steps_per_epoch=2000,\n",
    "             validation_steps=30,\n",
    "             epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aacbd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_embeddings = sg_model.layers[2].get_weights()[0]\n",
    "sg_embeddings = \n",
    "with open('sg_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(sg_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6fee7",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f75a93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches_cbow(sentences, window, batch_size=1000):\n",
    "    while True:\n",
    "        X_target = []\n",
    "        X_context = []\n",
    "        y = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            for i in range(len(sent)-1):\n",
    "                word = sent[i]\n",
    "                context = (sent[max(0, i - window // 2):i] + \n",
    "                           sent[i + 1:i + window // 2]) # окно разбивается на две половины\n",
    "\n",
    "                X_target.append(word)\n",
    "                X_context.append(context)\n",
    "                y.append(1)\n",
    "                \n",
    "                X_target.append(word2id[np.random.choice(\n",
    "                    filtered_vocab, p=word_probs)])  # негативные примеры выбираются пропорционально вероятностям\n",
    "                X_context.append(context)\n",
    "                y.append(0)\n",
    "\n",
    "                if len(X_target) == batch_size:\n",
    "                    X_target = np.array(X_target)\n",
    "                    X_context = tf.keras.preprocessing.sequence.pad_sequences(X_context, maxlen=window*2)\n",
    "                    y = np.array(y)\n",
    "                    yield ((X_target, X_context), y)\n",
    "                    X_target = []\n",
    "                    X_context = []\n",
    "                    y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "542e94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_target = tf.keras.layers.Input(shape=(1,))\n",
    "inputs_context = tf.keras.layers.Input(shape=(10,))\n",
    "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
    "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
    "target = tf.keras.layers.Flatten()(embeddings_target)\n",
    "context = tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))(embeddings_context)\n",
    "dot = tf.keras.layers.Dot(1)([target, context])\n",
    "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
    "model = tf.keras.Model(inputs=[inputs_target, inputs_context], \n",
    "                       outputs=outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e417c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2000/2000 [==============================] - 4921s 2s/step - loss: 0.6865 - accuracy: 0.5500 - val_loss: 0.6738 - val_accuracy: 0.6005\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - 4733s 2s/step - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6328 - val_accuracy: 0.6441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1364166caf0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(gen_batches_cbow(sentences, window=6), # размер окна изменён\n",
    "          validation_data=gen_batches_cbow(sentences,  window=6),\n",
    "          batch_size=1000,\n",
    "          steps_per_epoch=2000,\n",
    "          validation_steps=30,\n",
    "          epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e09e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_embeddings = model.layers[2].get_weights()[0]\n",
    "with open('cbow_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(cbow_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15b4472",
   "metadata": {},
   "source": [
    "## Сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2d4dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(word):\n",
    "    print(word)\n",
    "    print(f'Skip-gram: {most_similar(word, sg_embeddings)}\\nCBOW: {most_similar(word, cbow_embeddings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dab4c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "январь\n",
      "Skip-gram: ['январь', '2019', 'июнь', 'ноябрь', 'февраль', '2011', 'март', 'октябрь', '23', '1890']\n",
      "CBOW: ['январь', 'апрель', 'июнь', 'май', 'декабрь', 'октябрь', 'сентябрь', 'июль', 'март', '2009']\n"
     ]
    }
   ],
   "source": [
    "compare('январь')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5de0795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "война\n",
      "Skip-gram: ['война', 'отечественный', 'мировой', 'великий', 'армия', 'второй', 'пехотный', 'париж', '16-й', 'ход']\n",
      "CBOW: ['война', '1788', 'вступить', 'отечественный', '1978', 'русско-японский', '1994', 'мировой', '2003', '2004']\n"
     ]
    }
   ],
   "source": [
    "compare('война')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2bb94470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "город\n",
      "Skip-gram: ['город', 'центр', 'старое', 'провинция', 'долина', 'панихида', 'район', 'пункт', 'община', 'слияние']\n",
      "CBOW: ['город', 'переименовать', 'состав', 'поступить', 'частность', 'вступить', 'район', 'включить', 'участвовать', 'расположить']\n"
     ]
    }
   ],
   "source": [
    "compare('город')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82976205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "море\n",
      "Skip-gram: ['море', 'побережье', 'юг', 'северо-восточный', 'северный', 'центральный', 'север', 'восточный', 'юго-восточный', 'архангельский']\n",
      "CBOW: ['море', 'находиться', 'рука', 'км', 'fanuc', 'качество', 'восток', 'часть', 'paphiopedilum', 'борьба']\n"
     ]
    }
   ],
   "source": [
    "compare('море')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace677dc",
   "metadata": {},
   "source": [
    "Похоже, что skip-gram модель генерирует более правдоподобные эмбеддинги, вероятно, из-за большего размера окна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807362f4",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e42291e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "69e1007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_model = gensim.models.Word2Vec(preprocessed_texts,\n",
    "                                       vector_size=200, # этого вполне достаточно\n",
    "                                       min_count=0,\n",
    "                                       max_vocab_size=400000, # мы не хотим упускать никакие слова\n",
    "                                       window=4,\n",
    "                                       epochs=10, # нормальный баланс между качеством и скоростью\n",
    "                                       hs=1,\n",
    "                                       negative=0,\n",
    "                                       sample=1e-4,\n",
    "                                       ns_exponent=0,\n",
    "                                       cbow_mean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "11a1ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fasttext_model = gensim.models.FastText(preprocessed_texts,\n",
    "                                        vector_size=200, \n",
    "                                        min_count=0,\n",
    "                                        max_vocab_size=400000, \n",
    "                                        window=4,\n",
    "                                        epochs=10,\n",
    "                                        hs=1,\n",
    "                                        negative=0,\n",
    "                                        sample=1e-4,\n",
    "                                        ns_exponent=0,\n",
    "                                        cbow_mean=0,\n",
    "                                        min_n=4,\n",
    "                                        max_n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "69b4c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext_model.pickle', 'wb') as f:\n",
    "    pickle.dump(fasttext_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b2009ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('интерфейс', 0.6308376789093018),\n",
       " ('встроить', 0.5643938779830933),\n",
       " ('api', 0.5449751019477844),\n",
       " ('виртуальный', 0.5367855429649353),\n",
       " ('контроллер', 0.5338598489761353),\n",
       " ('core', 0.5334622263908386),\n",
       " ('ibm', 0.5180827975273132),\n",
       " ('клавиатура', 0.5154924392700195),\n",
       " ('файл', 0.5106635689735413),\n",
       " ('микроархитектура', 0.5063623189926147)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('процессор')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ac4ce659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('препроцессор', 0.9551441073417664),\n",
       " ('сопроцессор', 0.953107476234436),\n",
       " ('xinclude-процессор', 0.9361088275909424),\n",
       " ('кмоп-микропроцессор', 0.9199901819229126),\n",
       " ('микропроцессор', 0.905845046043396),\n",
       " ('видеопроцессор', 0.8963738083839417),\n",
       " ('platform.############процессор', 0.8771792054176331),\n",
       " ('процессорный', 0.8616329431533813),\n",
       " ('directx-видеопроцессор', 0.8447694778442383),\n",
       " ('наоборот.############процессор', 0.824285089969635)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('процессор')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "13565b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('самолёт', 0.6735202074050903),\n",
       " ('вертолёт', 0.5849144458770752),\n",
       " ('реактивный', 0.5836101174354553),\n",
       " ('корабль', 0.5830175280570984),\n",
       " ('ракетный', 0.571421205997467),\n",
       " ('модификация', 0.5659030079841614),\n",
       " ('снаряд', 0.5382475852966309),\n",
       " ('истребитель', 0.530783474445343),\n",
       " ('авианосец', 0.5228886008262634),\n",
       " ('машина', 0.522082507610321)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('ракета')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d938e96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ракета-торпеда', 0.8745078444480896),\n",
       " ('снаряд-ракета', 0.858798086643219),\n",
       " ('каракета', 0.8372753858566284),\n",
       " ('ракетчик', 0.8151965737342834),\n",
       " ('потакета', 0.8082267045974731),\n",
       " ('ракетоплан', 0.7970840334892273),\n",
       " ('ракето-торпеда', 0.7814498543739319),\n",
       " ('ракета-носитель', 0.7596801519393921),\n",
       " ('ракетоносец', 0.7411296367645264),\n",
       " ('ракета.############разработать', 0.7398077845573425)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('ракета')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
