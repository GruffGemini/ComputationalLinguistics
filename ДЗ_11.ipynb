{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ДЗ 11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1zFSD6HKkK7sfwr7KBweMP31vzoj4nF8r",
      "authorship_tag": "ABX9TyPV7grd2ULTH6DXa3RIE9Bs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f218ff600c14f75be8f951bacfe4cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83e16bd7c269461e9b4b6c6a6229b890",
              "IPY_MODEL_985839abd02340c9988582521acfe710",
              "IPY_MODEL_2c697f1072a3463cb862fe6fd2531f72"
            ],
            "layout": "IPY_MODEL_e7318e4aaa734dff90449a5725d3cecf"
          }
        },
        "83e16bd7c269461e9b4b6c6a6229b890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86df09db4fa9417e881778907740a0fa",
            "placeholder": "​",
            "style": "IPY_MODEL_f8b1bacd86d44c90a4a7dc33eeefbaa3",
            "value": "100%"
          }
        },
        "985839abd02340c9988582521acfe710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36c9abad09f7440290f4eb69499dc6a3",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cb7445526954a90ac9d49440417c60d",
            "value": 2000
          }
        },
        "2c697f1072a3463cb862fe6fd2531f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b57fbe8e0bb47048c98648d6bef996b",
            "placeholder": "​",
            "style": "IPY_MODEL_1fe249c396344a92ab3ddcc7f574d6ed",
            "value": " 2000/2000 [3:41:39&lt;00:00, 15.93s/it]"
          }
        },
        "e7318e4aaa734dff90449a5725d3cecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86df09db4fa9417e881778907740a0fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b1bacd86d44c90a4a7dc33eeefbaa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36c9abad09f7440290f4eb69499dc6a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb7445526954a90ac9d49440417c60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b57fbe8e0bb47048c98648d6bef996b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe249c396344a92ab3ddcc7f574d6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GruffGemini/ComputationalLinguistics/blob/main/%D0%94%D0%97_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1"
      ],
      "metadata": {
        "id": "0wUqy3d3JOUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers \n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB3blooM7BDc",
        "outputId": "9c33757b-d9d0-44d0-aac6-3fc8a789ce51"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 8.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 10.6 ms (started: 2022-04-28 12:09:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "YXmDc9LO653Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852e641a-38af-4169-eaea-4a00e70154d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.09 s (started: 2022-04-28 12:09:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9TZY8kOpsqZ",
        "outputId": "f5161bdc-a71b-4b6a-cf48-5effc5b4ece2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-28 12:09:55--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.de\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75998572 (72M)\n",
            "Saving to: ‘opus.de-en-train.de’\n",
            "\n",
            "opus.de-en-train.de 100%[===================>]  72.48M  19.7MB/s    in 4.6s    \n",
            "\n",
            "2022-04-28 12:10:01 (15.7 MB/s) - ‘opus.de-en-train.de’ saved [75998572/75998572]\n",
            "\n",
            "--2022-04-28 12:10:01--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70247384 (67M)\n",
            "Saving to: ‘opus.de-en-train.en’\n",
            "\n",
            "opus.de-en-train.en 100%[===================>]  66.99M  19.5MB/s    in 4.4s    \n",
            "\n",
            "2022-04-28 12:10:06 (15.2 MB/s) - ‘opus.de-en-train.en’ saved [70247384/70247384]\n",
            "\n",
            "--2022-04-28 12:10:06--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.de\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165307 (161K)\n",
            "Saving to: ‘opus.de-en-test.de’\n",
            "\n",
            "opus.de-en-test.de  100%[===================>] 161.43K   366KB/s    in 0.4s    \n",
            "\n",
            "2022-04-28 12:10:09 (366 KB/s) - ‘opus.de-en-test.de’ saved [165307/165307]\n",
            "\n",
            "--2022-04-28 12:10:09--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153644 (150K)\n",
            "Saving to: ‘opus.de-en-test.en’\n",
            "\n",
            "opus.de-en-test.en  100%[===================>] 150.04K   341KB/s    in 0.4s    \n",
            "\n",
            "2022-04-28 12:10:10 (341 KB/s) - ‘opus.de-en-test.en’ saved [153644/153644]\n",
            "\n",
            "time: 14.7 s (started: 2022-04-28 12:09:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.de\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.de\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.en"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "de_sents = open('opus.de-en-train.de').read().lower().splitlines()\n",
        "en_sents = open('opus.de-en-train.en').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "HfnaK2xuAqMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b79dc99-b186-4dad-d772-1a95b9246d6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.03 s (started: 2022-04-28 12:10:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(train_set_name: str):\n",
        "  tokenizer = Tokenizer(WordPiece(), )\n",
        "  tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
        "  tokenizer.train(files=[train_set_name], trainer=trainer)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "RdB2MQDg7Qzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213ac0a1-8715-444c-8c1d-78c782702b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.04 ms (started: 2022-04-28 07:22:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "tokenizer_de = train_tokenizer('opus.de-en-train.de')\n",
        "tokenizer_en = train_tokenizer('opus.de-en-train.en')\n",
        "tokenizer_de.save('tokenizer_de')\n",
        "tokenizer_en.save('tokenizer_en')\n",
        "\"\"\"\n",
        "tokenizer_en = Tokenizer.from_file(\"drive/MyDrive/CompLing 9/tokenizer_en\")\n",
        "tokenizer_de = Tokenizer.from_file(\"drive/MyDrive/CompLing 9/tokenizer_de\")"
      ],
      "metadata": {
        "id": "xLLmvmt_5B2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0a7ff8-e88d-47cf-a13a-d118199c765f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 816 ms (started: 2022-04-28 12:11:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[SEP]')]"
      ],
      "metadata": {
        "id": "eYYPCqBY_zXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794b31d9-dacd-4f0b-8891-77dbd8e2b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.24 ms (started: 2022-04-28 07:23:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
        "X_de = [encode(t, tokenizer_de) for t in de_sents]"
      ],
      "metadata": {
        "id": "lWKAr6W5AjkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cba2fac-8b26-4bbc-c4c3-8b88217b0c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1min 1s (started: 2022-04-28 07:23:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_de = int(np.mean([len(x) for x in X_de])) + 1\n",
        "max_len_en = int(np.mean([len(x) for x in X_en])) + 1\n",
        "PAD_IDX = tokenizer_en.token_to_id('[PAD]')"
      ],
      "metadata": {
        "id": "oebrMLplBqmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45546026-7e28-4691-b7f3-32488e01d778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 348 ms (started: 2022-04-28 07:24:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_de = tf.keras.preprocessing.sequence.pad_sequences(X_de, maxlen=max_len_de, padding='post', value=tokenizer_de.token_to_id('[PAD]'))\n",
        "\n",
        "X_en_out = tf.keras.preprocessing.sequence.pad_sequences([x[1:] for x in X_en], maxlen=max_len_en-1, padding='post', value=tokenizer_de.token_to_id('[PAD]'))\n",
        "\n",
        "X_en_dec = tf.keras.preprocessing.sequence.pad_sequences([x[:-1] for x in X_en], maxlen=max_len_en-1, padding='post', value=tokenizer_en.token_to_id('[PAD]'))"
      ],
      "metadata": {
        "id": "NR0j7p5XBzCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d5412d-a662-4b31-8000-57d64a59d772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 16.5 s (started: 2022-04-28 07:24:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_de_train, X_de_valid, X_en_dec_train, X_en_dec_valid, X_en_out_train, X_en_out_valid = train_test_split(X_de, X_en_dec, X_en_out, test_size=0.05)"
      ],
      "metadata": {
        "id": "veFFxaixEAs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263b79be-e2e0-48d3-a114-70006581708f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 383 ms (started: 2022-04-28 07:24:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    \"\"\"Calculate the attention weights. \"\"\"\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # add the mask to zero out padding tokens\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "7eECg1EpFqM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035e4f02-9f73-4c2c-b6a3-abf310ee02b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.43 ms (started: 2022-04-28 07:24:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # linear layers\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # split heads\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # concatenation of heads\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # final linear layer\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "QJajIualFu_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cfd9f9-2304-4e7e-d490-6b012a8c6c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 79 ms (started: 2022-04-28 07:24:54 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "7FIQ3u_DFxcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a343d488-1084-4fee-efe2-d7a8abab673f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.66 ms (started: 2022-04-28 07:24:57 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "xUK124fmFzL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454f8044-3bf6-4bfc-a372-4f713e2da0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.19 ms (started: 2022-04-28 07:25:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "cjS56yCfF0rY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89052ec4-73a0-4129-8bc8-4175840f8b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 20.3 ms (started: 2022-04-28 07:25:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "aGTngvk1F2Zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d913db-d96b-4b04-f010-47d8465d6341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.2 ms (started: 2022-04-28 07:25:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "5XgCRMIHF4dG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07335046-acc6-4f90-8a39-f188aaa921de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.3 ms (started: 2022-04-28 07:25:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "qbbDMhrgF6Wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124a4ea2-7faf-4cd4-85ca-ae5bf642d27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 22.7 ms (started: 2022-04-28 07:25:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "zU34dCU7F8jX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015ebbfe-25a3-435a-dc93-574643893cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 11.3 ms (started: 2022-04-28 07:25:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "    # mask the future tokens for decoder inputs at the 1st attention block\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "    # mask the encoder outputs for the 2nd attention block\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "      vocab_size=vocab_size[0],\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      max_len=max_len[0],\n",
        "    )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "      vocab_size=vocab_size[1],\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      max_len=max_len[1],\n",
        "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "Yar0l1GZF-fD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49760bb-fce2-4958-d30c-65983749e8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 21.1 ms (started: 2022-04-28 07:25:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none',)\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "ayCGX0cUGAzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794079e2-4e9f-4d47-d0a4-f13a22c5139c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.36 ms (started: 2022-04-28 07:25:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "5BZzM67MGC48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5818345a-4be9-4122-eb2d-c63893859029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.62 ms (started: 2022-04-28 07:25:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "with mirrored_strategy.scope():\n",
        "    model = transformer(\n",
        "        vocab_size=(tokenizer_de.get_vocab_size(),tokenizer_en.get_vocab_size()),\n",
        "        num_layers=NUM_LAYERS,\n",
        "        units=UNITS,\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dropout=DROPOUT,\n",
        "        max_len=[max_len_de, max_len_en])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('drive/MyDrive/CompLing 9/model_deen',\n",
        "                                                monitor='val_loss',\n",
        "                                                verbose=1,\n",
        "                                            save_weights_only=True,\n",
        "                                            save_best_only=True,\n",
        "                                            mode='min',\n",
        "                                            save_freq='epoch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OasLwMdZGElW",
        "outputId": "a7e450a6-b0c6-4386-d0f0-529f81c3f76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "time: 7.2 s (started: 2022-04-28 07:25:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''model.fit((X_de_train, X_en_dec_train), X_en_out_train, \n",
        "             validation_data=((X_de_valid, X_en_dec_valid), X_en_out_valid),\n",
        "             batch_size=200,\n",
        "             epochs=10,\n",
        "             callbacks=[checkpoint]\n",
        "             )'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIbwFuuOGeJZ",
        "outputId": "c6666988-de99-453a-d6fc-db5705a0592e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 2.5217 - accuracy: 0.2974\n",
            "Epoch 1: val_loss improved from inf to 1.99523, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1372s 287ms/step - loss: 2.5217 - accuracy: 0.2974 - val_loss: 1.9952 - val_accuracy: 0.3570\n",
            "Epoch 2/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.9222 - accuracy: 0.3597\n",
            "Epoch 2: val_loss improved from 1.99523 to 1.84235, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1362s 287ms/step - loss: 1.9222 - accuracy: 0.3597 - val_loss: 1.8424 - val_accuracy: 0.3729\n",
            "Epoch 3/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.7984 - accuracy: 0.3720\n",
            "Epoch 3: val_loss improved from 1.84235 to 1.78247, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1372s 289ms/step - loss: 1.7984 - accuracy: 0.3720 - val_loss: 1.7825 - val_accuracy: 0.3797\n",
            "Epoch 4/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.7315 - accuracy: 0.3787\n",
            "Epoch 4: val_loss improved from 1.78247 to 1.74732, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1377s 290ms/step - loss: 1.7315 - accuracy: 0.3787 - val_loss: 1.7473 - val_accuracy: 0.3834\n",
            "Epoch 5/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.6861 - accuracy: 0.3834\n",
            "Epoch 5: val_loss improved from 1.74732 to 1.72479, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1376s 290ms/step - loss: 1.6861 - accuracy: 0.3834 - val_loss: 1.7248 - val_accuracy: 0.3857\n",
            "Epoch 6/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.6534 - accuracy: 0.3869\n",
            "Epoch 6: val_loss improved from 1.72479 to 1.71148, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1387s 292ms/step - loss: 1.6534 - accuracy: 0.3869 - val_loss: 1.7115 - val_accuracy: 0.3884\n",
            "Epoch 7/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.6273 - accuracy: 0.3896\n",
            "Epoch 7: val_loss improved from 1.71148 to 1.70534, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1375s 290ms/step - loss: 1.6273 - accuracy: 0.3896 - val_loss: 1.7053 - val_accuracy: 0.3895\n",
            "Epoch 8/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.6080 - accuracy: 0.3917\n",
            "Epoch 8: val_loss improved from 1.70534 to 1.69939, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1376s 290ms/step - loss: 1.6080 - accuracy: 0.3917 - val_loss: 1.6994 - val_accuracy: 0.3898\n",
            "Epoch 9/10\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 1.5905 - accuracy: 0.3936\n",
            "Epoch 9: val_loss improved from 1.69939 to 1.69790, saving model to drive/MyDrive/CompLing 9/model_deen\n",
            "4750/4750 [==============================] - 1376s 290ms/step - loss: 1.5905 - accuracy: 0.3936 - val_loss: 1.6979 - val_accuracy: 0.3917\n",
            "Epoch 10/10\n",
            "4099/4750 [========================>.....] - ETA: 3:04 - loss: 1.5725 - accuracy: 0.3956"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('drive/MyDrive/CompLing 9/model_deen')\n",
        "#loss, acc = model.evaluate((X_de_valid, X_en_dec_valid), X_en_out_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-sQHiCckPFI",
        "outputId": "edf2f729-9fce-4a02-ff9f-20e4a974307a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f104c39c490>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.56 s (started: 2022-04-28 07:25:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    input_ids = encode(text.lower(), tokenizer_de)\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences([input_ids], maxlen=max_len_de, padding='post')\n",
        "    output_ids = [tokenizer_en.token_to_id('[CLS]') ] \n",
        "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False) \n",
        "\n",
        "    while pred.numpy().argmax(2)[0][-1] not in [tokenizer_en.token_to_id('[SEP]')]:\n",
        "      if len(output_ids) > max_len_en or not pred.numpy().argmax(2)[0][-1]:\n",
        "        break\n",
        "      output_ids.append(pred.numpy().argmax(2)[0][-1])\n",
        "      pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
        "\n",
        "    return tokenizer_en.decode(output_ids[1:])"
      ],
      "metadata": {
        "id": "wrTk6bMpFxUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8326e57f-1c7c-4998-87a1-5086eaf7eef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.53 ms (started: 2022-04-28 07:25:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_translate(batch):\n",
        "    input_ids = [encode(text.lower(), tokenizer_de) for text in batch]\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_len_de, padding='post', truncating='post')\n",
        "    output_ids = [[tokenizer_en.token_to_id('[CLS]')] for _ in range(len(input_ids))] \n",
        "    batch_pred = model((input_ids, tf.cast(output_ids, tf.int32)), training=False)\n",
        "\n",
        "    while True:\n",
        "      if all(\n",
        "          pred.numpy().argmax(1)[-1] in [tokenizer_en.token_to_id('[SEP]')] or \n",
        "          len(output_id) >= max_len_en or \n",
        "          not pred.numpy().argmax(1)[-1]\n",
        "          \n",
        "          for output_id, pred in zip(output_ids, batch_pred)\n",
        "        ):\n",
        "        break\n",
        "      for i in range(len(output_ids)):\n",
        "        output_ids[i].append(batch_pred[i].numpy().argmax(1)[-1])\n",
        "      batch_pred = model((input_ids, tf.cast(output_ids, tf.int32)), training=False) \n",
        "\n",
        "    return [tokenizer_en.decode(output_id[1:]) for output_id in output_ids]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5pLQozFgq_s",
        "outputId": "714dfa3d-894d-4181-87bd-3ae5baba5e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.1 ms (started: 2022-04-28 07:25:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(\"du hast mich gefragt und ich hab' nichts gesagt\"))\n",
        "print(translate(\"Neunundneunzig Luftballons Auf ihrem Weg zum Horizont\"))\n",
        "print(translate(\"All diese Momente werden verloren sein in der Zeit, so wie Tränen im Regen\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZKOP8UBmg6Q",
        "outputId": "08c9f5ad-77f1-4558-c1f0-6e29bd76b934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you asked me and i didn ' t say\n",
            "nine and new air trainees on your way to the horizon\n",
            "all these moments are lost in time , like tears in the rain\n",
            "time: 6.6 s (started: 2022-04-28 07:25:58 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(batch_translate([\"du hast mich gefragt und ich hab' nichts gesagt\",\n",
        "                        \"Neunundneunzig Luftballons Auf ihrem Weg zum Horizont\",\n",
        "                        \"All diese Momente werden verloren sein in der Zeit, so wie Tränen im Regen\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXE_tapzmJDj",
        "outputId": "b55c64a1-6963-49d8-bf85-cbfd3a278b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"you asked me and i didn ' t say\",\n",
            " 'nine and new air trainees on your way to the horizon',\n",
            " 'all these moments are lost in time , like tears in the rain']\n",
            "time: 1.92 s (started: 2022-04-28 07:26:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pickle\n",
        "\n",
        "BATCH_SIZE = 500\n",
        "SKIP = 1200\n",
        "\n",
        "translations = []\n",
        "with open(f'drive/MyDrive/CompLing 9/translations_{SKIP}.pickle', 'rb') as f:\n",
        "  translations = pickle.load(f)\n",
        "\n",
        "for index, batch in enumerate(tqdm([de_sents[i:i + BATCH_SIZE] for i in range(0, len(de_sents), BATCH_SIZE)])):\n",
        "  if index <= SKIP:\n",
        "    continue\n",
        "  try:\n",
        "    translations.extend(batch_translate(batch))\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(batch)\n",
        "  if not index % 100:\n",
        "    filename = f'drive/MyDrive/CompLing 9/translations_{index}.pickle'\n",
        "    with open(filename, 'wb') as f:\n",
        "      pickle.dump(translations, f)\n",
        "      print(f'translations saved as {filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "0f218ff600c14f75be8f951bacfe4cbe",
            "83e16bd7c269461e9b4b6c6a6229b890",
            "985839abd02340c9988582521acfe710",
            "2c697f1072a3463cb862fe6fd2531f72",
            "e7318e4aaa734dff90449a5725d3cecf",
            "86df09db4fa9417e881778907740a0fa",
            "f8b1bacd86d44c90a4a7dc33eeefbaa3",
            "36c9abad09f7440290f4eb69499dc6a3",
            "9cb7445526954a90ac9d49440417c60d",
            "2b57fbe8e0bb47048c98648d6bef996b",
            "1fe249c396344a92ab3ddcc7f574d6ed"
          ]
        },
        "id": "oIgmmbHpe_xU",
        "outputId": "81f3166e-ba06-499a-9fbc-26cd6a5595d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f218ff600c14f75be8f951bacfe4cbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "translations saved as drive/MyDrive/CompLing 9/translations_1300.pickle\n",
            "translations saved as drive/MyDrive/CompLing 9/translations_1400.pickle\n",
            "translations saved as drive/MyDrive/CompLing 9/translations_1500.pickle\n",
            "translations saved as drive/MyDrive/CompLing 9/translations_1600.pickle\n",
            "translations saved as drive/MyDrive/CompLing 9/translations_1700.pickle\n",
            "translations saved as drive/MyDrive/CompLing 9/translations_1800.pickle\n",
            "translations saved as drive/MyDrive/CompLing 9/translations_1900.pickle\n",
            "time: 3h 41min 41s (started: 2022-04-28 07:29:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/CompLing 9/translations', 'wb') as f:\n",
        "  pickle.dump(translations, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRF7bdz73pGO",
        "outputId": "132e1e50-6abb-4233-9fb9-f42d053aea49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 697 ms (started: 2022-04-28 11:12:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('drive/MyDrive/CompLing 9/translations', 'rb') as f:\n",
        "  translations = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzr1f4u3EB6x",
        "outputId": "4262fd9e-e50c-4069-e6e7-30025bf0dc73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 935 ms (started: 2022-04-28 12:10:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(de_sents))\n",
        "print(len(translations))\n",
        "print(de_sents[4243])\n",
        "print(translations[4243])"
      ],
      "metadata": {
        "id": "isQ4F6nwCpY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ac2083-45df-4ddf-b7ec-29e11f17158f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000\n",
            "1000000\n",
            "ich bitte dich, du kannst doch nicht ernsthaft in ihn verliebt sein, oder?\n",
            "i beg you , can ' t you seriously fall in love with him , right ?\n",
            "time: 131 ms (started: 2022-04-28 12:10:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "  reference = tokenizer_en.encode(t).tokens\n",
        "  hypothesis = tokenizer_en.encode(en_sents[i]).tokens\n",
        "\n",
        "bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1qjYA6mFHel",
        "outputId": "ffcdf4ed-36bb-433c-ecd9-d68942311298"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 44.2 s (started: 2022-04-28 12:15:14 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(bleus)/len(bleus))*100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c0oVZ4_F8lk",
        "outputId": "ed5cd72e-770b-400b-9a6f-057215401b74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.28736642487601"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.24 ms (started: 2022-04-28 12:16:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2"
      ],
      "metadata": {
        "id": "AO6gv0fvUvOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прием backtranslation применяется в том случае, если имеется недостаточно параллельных текстов (то есть обучающая выборка, где каждому тексту языка-источника сопоставляется текст целевого языка, слишком мала). Backtranslation позволяет искусственно увеличить объем обучающей выборки путем генерации дополнительных пар текстов. \n",
        "\n",
        "Техника применима в том случае, если для целевого языка имеется достаточно большое число текстов (без соответствия на языке-источнике). В этом случае строится вспомогательная модель машинного перевода, обучаемая на имеющейся обучающей выборке, но в обратную сторону. То есть, модель обучается переводить тексты с целевого языка на язык-источник. Тогда, взяв сколь угодно много текстов на целевом языке, можно сгенерировать такое же большое число текстов на языке-источнике, после чего использовать полученные пары при обучении основной модели.\n",
        "\n",
        "Если рассматривать пример из семинара, прием backtranslation был бы применим в том гипотетическом случае, если бы имелось много текстов на русском языке, но мало на английском. Тогда на имеющихся данных можно было бы обучить простой трансформер, который бы переводил предложения с русского на английский. Затем следовало бы из какого-либо другого источника получить множество текстов на русском языке и прогнать на них вспомогательный трансформер, получив столько же искусственных текстов на английском языке. Таким образом, было бы получено расширение обучающей выборки, и уже на этой расширенной выборке можно было бы обучать основной трансформер, осуществляющий перевод с английского на русский."
      ],
      "metadata": {
        "id": "29U-OusxQZWv"
      }
    }
  ]
}