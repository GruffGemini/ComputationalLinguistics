{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7458fb3a",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5ec45",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4f28ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зависимости кода, указанного в задании\n",
    "\n",
    "import re\n",
    "import textdistance\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
    "    # 1 вектора ко всей матрице (словам в словаре)\n",
    "    # считать по отдельности циклом было бы дольше\n",
    "    # вместо одного вектора может даже целая матрица\n",
    "    # тогда считаться в итоге будет ещё быстрее\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]\n",
    "\n",
    "with open('wiki_data.txt', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "vocab = Counter(re.findall(r'\\w+', corpus.lower()))\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=1000)\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a6ac468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код, указанный в задании\n",
    "\n",
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    # Counter можно использовать и с не целыми числами\n",
    "    similarities = Counter()\n",
    "    \n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    \n",
    "    return closest\n",
    "\n",
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "59bd9fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('паровоз', 0.8571428571428572, -11.60538419447003),\n",
       " ('паровой', 0.8571428571428572, -11.900183734690675),\n",
       " ('паровом', 0.8571428571428572, -14.356919507511979),\n",
       " ('паровое', 0.8571428571428572, -14.762384615620144),\n",
       " ('паровоза', 0.75, -12.236655971311889),\n",
       " ('паровозы', 0.75, -12.236655971311889),\n",
       " ('парового', 0.75, -13.846093883745988),\n",
       " ('паровозу', 0.75, -14.069237435060199),\n",
       " ('парковое', 0.75, -14.762384615620144),\n",
       " ('парчовое', 0.75, -15.455531796180088),\n",
       " ('паровая', 0.7142857142857143, -12.565160038283924),\n",
       " ('паровые', 0.7142857142857143, -12.682943073940308),\n",
       " ('паровую', 0.7142857142857143, -13.25830721884387),\n",
       " ('паров', 0.7142857142857143, -13.376090254500253),\n",
       " ('шаровой', 0.7142857142857143, -13.509621647124774),\n",
       " ('карово', 0.7142857142857143, -14.762384615620144),\n",
       " ('паромов', 0.7142857142857143, -14.762384615620144),\n",
       " ('даровой', 0.7142857142857143, -15.455531796180088),\n",
       " ('парво', 0.7142857142857143, -15.455531796180088),\n",
       " ('паровым', 0.7142857142857143, -15.455531796180088)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Код решения задания\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn * 4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    closest_with_probabilities = [(word[0], word[1], np.log(P(word[0]))) for word in closest]\n",
    "    # добавляем вероятности к результату\n",
    "\n",
    "    return sorted(closest_with_probabilities, key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    # сортируем сначала по расстоянию редактирования, затем по вероятности\n",
    "\n",
    "get_closest_hybrid_match('паровощ', X, vec, 20)\n",
    "# среди кандидатов с одинаковым расстоянием редактирования наиболее вероятные всегда будут первыми"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "af2fd79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'паровоз'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# или если нужно только слово:\n",
    "get_closest_hybrid_match('паровощ', X, vec, 20)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c230b37",
   "metadata": {},
   "source": [
    "## 2. Symspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3be06749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Составляем словарь правильных слов\n",
    "with open('wiki_data.txt', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "vocab = Counter(re.findall(r'\\w+', corpus.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d7ed146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составляем словарь удалений\n",
    "deletions_vocab = defaultdict(list)\n",
    "for word in vocab:\n",
    "    for index, _ in enumerate(word):\n",
    "        deletions_vocab[word[0:index] + word[index + 1:]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d2b83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symspell(bad_word: str, deletions: dict, main_vocab: dict) -> str:\n",
    "    if bad_word in main_vocab:\n",
    "        return bad_word\n",
    "    candidates = set()\n",
    "    for i, _ in enumerate(bad_word):\n",
    "        candidates.update(deletions[bad_word[0:i] + bad_word[i + 1:]])\n",
    "    candidates = sorted(candidates, key=lambda x: main_vocab[x], reverse=True) # сортируем кандидатов по вероятности\n",
    "    try:\n",
    "        return candidates[0]\n",
    "    except IndexError:\n",
    "        return bad_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21906fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'паровоз'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symspell('паровощ', deletions_vocab, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7228af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
    "\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f635e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества\n",
    "def evaluate(correction):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    total_mistaken = 0\n",
    "    mistaken_fixed = 0\n",
    "\n",
    "    total_correct = 0\n",
    "    correct_broken = 0\n",
    "\n",
    "    cashed = {}\n",
    "    for i in range(len(true)):\n",
    "        word_pairs = align_words(true[i], bad[i])\n",
    "        for pair in word_pairs:\n",
    "            # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "            # перед тем как считать исправление проверим нет ли его в кеше\n",
    "\n",
    "            predicted = cashed.get(pair[1], correction(pair[1], deletions_vocab, vocab))\n",
    "            cashed[pair[1]] = predicted\n",
    "\n",
    "\n",
    "            if predicted == pair[0]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            if pair[0] == pair[1]:\n",
    "                total_correct += 1\n",
    "                if pair[0] !=  predicted:\n",
    "                    correct_broken += 1\n",
    "            else:\n",
    "                total_mistaken += 1\n",
    "                if pair[0] == predicted:\n",
    "                    mistaken_fixed += 1\n",
    "    return {'correct':correct, 'total':total, 'mistaken_fixed': mistaken_fixed, 'total_mistaken':total_mistaken, \n",
    "            'correct_broken': correct_broken, 'total_correct':total_correct}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ee732bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855927963981991\n",
      "0.19875776397515527\n",
      "0.04685884920179166\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(symspell)\n",
    "print(result['correct']/result['total'])\n",
    "print(result['mistaken_fixed']/result['total_mistaken'])\n",
    "print(result['correct_broken']/result['total_correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ba511",
   "metadata": {},
   "source": [
    "Результат не очень хороший - исправляются только 20% ошибок. Можно попробовать увеличить процент, если перед генерацией удалений записать в список кандидатов варианты исправлений для самого ошибочного слова в неизменном виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "34631499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symspell_modified(bad_word: str, deletions: dict, main_vocab: dict) -> str:\n",
    "    if bad_word in main_vocab:\n",
    "        return bad_word\n",
    "    candidates = set(deletions[bad_word]) # это делается здесь\n",
    "    for i, _ in enumerate(bad_word):\n",
    "        candidates.update(deletions[bad_word[0:i] + bad_word[i + 1:]])\n",
    "    candidates = sorted(candidates, key=lambda x: main_vocab[x], reverse=True) # сортируем кандидатов по вероятности\n",
    "    try:\n",
    "        return candidates[0]\n",
    "    except IndexError:\n",
    "        return bad_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ba4aa26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8680340170085042\n",
      "0.30667701863354035\n",
      "0.04892615137245894\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(symspell_modified)\n",
    "print(result['correct']/result['total'])\n",
    "print(result['mistaken_fixed']/result['total_mistaken'])\n",
    "print(result['correct_broken']/result['total_correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ec202",
   "metadata": {},
   "source": [
    "Уже лучше, но всё равно только 30%. Так как в чистом виде в алгоритме создаётся только словарь удалений, вряд ли удастся серьёзно улучшить результат без значительных изменений алгоритма"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
